# -*- coding: utf-8 -*-
"""fakenewsdetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RWWefP3df_fPSBmDpXtoF47Bhw4cqWwe
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
train= pd.read_csv('/content/train.csv',sep='\t',encoding='utf-8') 
test = pd.read_csv("/content/test.csv",sep='\t',encoding='utf-8')

train.head()

"""## **Bag of Words** **Pipeline**"""

import nltk #natural language toolkit
nltk.download('punkt')
nltk.download('stopwords')

train['label'].loc[0]

train['text'].loc[0]#returns the very first article

from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
def preprocess_data(data):
   #1.Tokenization
   tk= RegexpTokenizer('/s+', gaps=True) #'/s---->Space'
   text_data=[]#List for storing the tokenized data
   for values in data.text:
     tokenized_data = tk.tokenize(values)
     text_data.append(tokenized_data)
  #stopwords removal
   sw=stopwords.words('english')
   clean_data =[]
   for data in text_data:
     clean_text=[words.lower()for words in data if words.lower()not in sw]
     clean_data.append(clean_text)

    #3.#stemming 
    #Create aStemmer object
   ps=PorterStemmer()
   stemmed_data = [] #list for storing the stmmed data
   for data in clean_data:
      stemmed_text = [ps.stem(words) for words in data]#stemthe words
      stemmed_data.append(stemmed_text)
   #4.tfidfvectorization
   updated_data =[]
   for data in stemmed_data:
     updated_data.append(" ".join(data))
   #TFID vector onject
   tfidf = TfidfVectorizer()
   tfidf_matrix = tfidf.fit_transform(updated_data)
   return tfidf_matrix

train_len = train.shape[0]
merged_data = pd.concat((train.drop('label',axis=1), test.drop('id',axis=1)), axis=0).reset_index().drop('index', axis=1)

preprocessed_data = preprocess_data(merged_data)

train_data = preprocessed_data[: train_len]
test_data = preprocessed_data[train_len :]

"""# Modelling"""

from sklearn.model_selection import train_test_split
 X_train, X_test, y_train, y_test = train_test_split(train_data, train.label, test_size=0.2, random_state = 42)

from sklearn.metrics import accuracy_score

# model
def compute_metrics(data, y_true, model_obj, model):

  # Make predictions
  y_pred = model_obj.predict(data)

  # Compute accuracy
  acc = accuracy_score(y_true = y_true, y_pred = y_pred)

  # Make DataFrame
  metrics = pd.DataFrame(data = np.array([acc]), index=[model], columns=['Accuracy Score'])
  return metrics

from sklearn.linear_model import LogisticRegressionCV

#modelobject
lr_reg = LogisticRegressionCV(Cs=20,cv=3,random_state=42)

#fit the model 
lr_reg.fit(X_train,y_train)

# Compute the Logistic Regression Metrics
lr_metrics =  compute_metrics(X_test, y_test, lr_reg, 'LogisticRegression')

lr_metrics_train =  compute_metrics(X_train, y_train, lr_reg, 'LogisticRegression')

lr_metrics

lr_metrics_train

from sklearn.naive_bayes import MultinomialNB

# Model Object
mnb = MultinomialNB(alpha=0.0)

# Fit the object
mnb.fit(X_train, y_train)

mnb_metrics = compute_metrics(X_test, y_test, mnb, 'Naive Bayes')
mnb_metrics

from sklearn.tree import DecisionTreeClassifier

# Model Object
dt_clf = DecisionTreeClassifier()

# Fit the object
dt_clf.fit(X_train, y_train)

dt_metrics = compute_metrics(X_test,y_test,dt_clf, "DecisionTree")

dt_metrics

from xgboost import XGBClassifier

# XGB model
xgb_model = XGBClassifier(n_estimators=200)

xgb_model.fit(X_train, y_train)

xgb_metrics = compute_metrics(X_test, y_test, xgb_model, 'XGBClassifier')

model_metrics = pd.concat((lr_metrics, mnb_metrics, dt_metrics, xgb_metrics), axis=0).sort_values(by='Accuracy Score', ascending=False)

model_metrics

"""### MAKING **Predictions**

XGB PREdictions
"""

# Make predictions --> XGBoost
predictions = xgb_model.predict(test_data)
# Submissions
test_ID = test.id
submissions = pd.DataFrame({'id' : test_ID, 'label' : predictions})

submissions.head()

submissions.to_csv("/content/predictions", index=False)